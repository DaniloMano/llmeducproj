# ğŸ¤– Gemini - Guia Completo de PersonalizaÃ§Ã£o

Este repositÃ³rio contÃ©m um guia completo sobre como usar e **personalizar modelos Gemini**, da Google, para aplicaÃ§Ãµes prÃ¡ticas como assistentes virtuais, chatbots empresariais, anÃ¡lise de documentos e muito mais.

> ğŸ¥ **Veja a playlist de tutoriais aqui:** [ğŸ”— Link para a playlist no YouTube](https://youtube.com/playlist?list=PL-JHMiljvr-q1XBjPA-6wNkIDz7IQin8h&si=d9s2GUagscSufFvO)

---

## ğŸ“Œ O que Ã© o Gemini?

O **Gemini** Ã© a famÃ­lia de modelos de linguagem desenvolvida pela Google DeepMind, sucessora do PaLM 2, projetada para ser **multimodal**, **eficiente** e **altamente escalÃ¡vel**. Ã‰ utilizado em produtos como Bard, Android (via Gemini Nano), Google Workspace e Vertex AI.

---

## ğŸ§  Formas de personalizar o Gemini

VocÃª pode adaptar o Gemini aos seus prÃ³prios dados e contexto de vÃ¡rias formas:

### 1. ğŸ“„ Prompt Engineering
A forma mais rÃ¡pida de personalizaÃ§Ã£o. Consiste em escrever instruÃ§Ãµes e exemplos diretamente no prompt para guiar o comportamento do modelo.

âœ… Simples  
âœ… NÃ£o precisa de programaÃ§Ã£o  
âŒ NÃ£o Ã© persistente entre sessÃµes

### 2. ğŸ” RAG (Retrieval-Augmented Generation)
TÃ©cnica que combina LLM com uma **base de dados vetorizada** para respostas fundamentadas em fontes externas (como documentos, PDFs, banco de dados etc.).

âœ… Usa dados atualizados  
âœ… Evita hallucinations  
âœ… Ideal para dados dinÃ¢micos

### 3. ğŸ¯ Fine-Tuning (Ajuste Fino)
Treinamento supervisionado com seus prÃ³prios exemplos de entrada e saÃ­da. Ideal para adaptar o modelo Ã  linguagem da sua empresa ou casos muito especÃ­ficos.

âœ… Muito preciso  
âœ… Respostas padronizadas  
âŒ Mais caro e demorado  
âŒ Pode exigir infraestrutura com GPU

---

## ğŸ¯ Fine-Tuning com Gemini

**Fine-Tuning** Ã© o processo de ajustar o modelo Gemini com dados especÃ­ficos da sua empresa, produto ou domÃ­nio de conhecimento. Diferente do prompt engineering (que sÃ³ afeta uma Ãºnica interaÃ§Ã£o), o fine-tuning cria um modelo customizado que **aprende padrÃµes, linguagem e estilo** diretamente dos seus dados.

---

### ğŸ“š Para que serve o Fine-Tuning?

- Criar um assistente que responda com a linguagem da sua marca
- Padronizar respostas para atendimento ao cliente
- Ensinar o modelo a interpretar regras ou documentos tÃ©cnicos
- Reduzir a necessidade de fornecer exemplos longos no prompt

---

### ğŸ—‚ï¸ Como preparar os dados para o Fine-Tuning?

O modelo espera **pares de entrada e saÃ­da** (input/output), geralmente no formato `.jsonl`.

ğŸ“ **Exemplo:**

```json
{ "input": "Quais sÃ£o os documentos necessÃ¡rios para admissÃ£o?", "output": "VocÃª precisarÃ¡ do RG, CPF, comprovante de residÃªncia e carteira de trabalho." }
```

## ğŸ› ï¸ Como treinar o Gemini com seus dados (Fine-Tuning)

### Passo a passo bÃ¡sico:

1. Prepare seus dados de treinamento (pares de entrada/saÃ­da em JSONL).
2. Acesse o [Google AI Studio](https://makersuite.google.com/) ou o [Vertex AI](https://console.cloud.google.com/vertex-ai).
3. Crie um novo projeto com faturamento habilitado (pois alguns tipos de contas nÃ£o conseguem ver a opÃ§Ã£o "new tuned model" no Google AI Studio).
4. Use a opÃ§Ã£o **"Tuned Model"** para carregar seus dados e treinar um modelo customizado.
5. Caso a opÃ§Ã£o nÃ£o esteja visÃ­vel, siga o caminho alternativo abaixo.

---

### ğŸ”„ CenÃ¡rio alternativo: "Tuned Model" nÃ£o aparece no AI Studio?

Se vocÃª **nÃ£o vÃª a opÃ§Ã£o "New tuned model"** no AI Studio, pode ser por conta de:

- Tipo de conta (pessoal vs. corporativa)
- ConfiguraÃ§Ã£o de faturamento
- RegiÃ£o ou limitaÃ§Ãµes da interface

âœ… **SoluÃ§Ã£o alternativa: utilize o [Vertex AI](https://www.youtube.com/watch?v=ej_ZUcyKpoc&list=PL-JHMiljvr-q1XBjPA-6wNkIDz7IQin8h&index=7) (via Google Cloud)**

ğŸ“º **Ou siga este vÃ­deo tutorial da prÃ³pria Google para realizar o fine-tuning:**

[ğŸ“¹ Tutorial - Fine-tuning do Gemini na prÃ¡tica](https://www.youtube.com/watch?v=-ja5TmYhQks&list=PL-JHMiljvr-q1XBjPA-6wNkIDz7IQin8h&index=1)

---

## ğŸ“ Exemplo de dado de treino (JSONL)

```json
{ "input": "Qual o horÃ¡rio de atendimento?", "output": "Nosso horÃ¡rio de atendimento Ã© das 8h Ã s 17h, de segun``da a sexta." }
```

## ğŸ” RAG - Retrieval-Augmented Generation

**RAG (GeraÃ§Ã£o com RecuperaÃ§Ã£o Aumentada)** Ã© uma tÃ©cnica que permite que o modelo Gemini consulte uma base de dados ou documentos externos para gerar respostas mais precisas e atualizadas.

Em vez de depender apenas da memÃ³ria do modelo, o RAG faz uma **busca vetorial em uma base de conhecimento externa**, trazendo os trechos mais relevantes do conteÃºdo para serem incluÃ­dos no prompt da LLM.

### ğŸ“¦ Como funciona o RAG:

1. ğŸ§¾ O usuÃ¡rio faz uma pergunta.
2. ğŸ” Um mecanismo de busca vetorial (como **Vertex AI Matching Engine**, **Weaviate**, **Pinecone**, **FAISS**, etc.) encontra os trechos de documentos mais relevantes.
3. ğŸ§  Esses trechos sÃ£o incluÃ­dos como contexto no prompt do Gemini.
4. ğŸ’¬ O Gemini responde com base no conteÃºdo recuperado + seu conhecimento prÃ©vio.

---

### âœ… Vantagens do RAG

- Permite que o modelo "leia" documentos grandes sem re-treinamento.
- Pode ser atualizado dinamicamente (basta atualizar os documentos).
- Reduz hallucinations, pois a resposta Ã© ancorada em fontes reais.

---

### ğŸ› ï¸ Exemplo prÃ¡tico com Google Cloud

A prÃ³pria Google demonstrou como implementar RAG no Vertex AI, utilizando:

- **Gemini API**
- **Vertex AI Embeddings**
- **Vertex AI Matching Engine** (base vetorial)
- IntegraÃ§Ã£o com **Firestore** para armazenar histÃ³rico de conversas

ğŸ“º **Veja o tutorial oficial aqui:**

[ğŸ“¹ Como usar RAG com Gemini no Vertex AI (YouTube)](https://www.youtube.com/watch?v=zckcxStWHzQ)

---

## ğŸ§  CAG - Cache-Augmented Generation

O **CAG (Cache-Augmented Generation)** Ã© uma tÃ©cnica complementar ao RAG (Retrieval-Augmented Generation) que busca **aumentar a eficiÃªncia, reduzir custos e acelerar respostas** em sistemas com LLMs como o Gemini.

Enquanto o RAG consulta bases externas (como documentos vetorizados), o CAG adiciona uma camada **anterior ou posterior** com **cache** de perguntas e respostas, permitindo que o sistema **reutilize interaÃ§Ãµes anteriores** sempre que possÃ­vel.

---

### âš™ï¸ Como funciona o CAG?

```mermaid
graph TD
    A[UsuÃ¡rio envia pergunta] --> B{Cache contÃ©m algo similar?}
    B -- Sim --> C[Retorna resposta diretamente do cache]
    B -- NÃ£o --> D["RAG (busca vetorial)"]
    D --> E[LLM gera resposta]
    E --> F[Salva no cache]
    F --> G[Retorna resposta ao usuÃ¡rio]
```
- O cache pode ser de strings exatas ou semÃ¢nticas (perguntas parecidas).

- Pode ser implementado com ferramentas como Redis, FAISS, Qdrant, etc.

- Ideal para perguntas repetitivas, como FAQs, suporte tÃ©cnico, atendimento ao cliente, etc.

âœ… BenefÃ­cios do CAG
âš¡ Desempenho: respostas quase instantÃ¢neas apÃ³s a primeira consulta

ğŸ’¸ Economia: reduz chamadas repetidas ao modelo LLM (especialmente Ãºtil em APIs pagas)

ğŸ§  InteligÃªncia: pode ser combinado com buscas vetoriais e contexto semÃ¢ntico

ğŸ”„ Escalabilidade: Ã³timo para sistemas de atendimento com alto volume de uso

ğŸ’¡ Exemplo de AplicaÃ§Ã£o
Imagine um chatbot empresarial que responde frequentemente:

"Qual o horÃ¡rio de atendimento?"

Na primeira vez, a resposta Ã© gerada por meio do RAG + LLM. Depois disso, sempre que alguÃ©m fizer essa mesma pergunta (ou variaÃ§Ãµes parecidas), a resposta serÃ¡ recuperada instantaneamente do cache, economizando tempo e dinheiro.

ğŸ“º [ReferÃªncia](https://www.youtube.com/watch?v=KHDMoQ2Sp2s&t=530s)